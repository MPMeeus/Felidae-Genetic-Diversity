shell.prefix("source /data/antwerpen/grp/asvardal/share/hscon5_setup.sh; ")
# TEST bcftools on Silversides Variant Calling
# The main entry point of your workflow.
# After configuring, running snakemake -n in a clone of this repository should successfully execute a dry-run of the workflow.

import subprocess
from scipy import stats
import pandas as pd
import os, yaml
os.environ['OPENBLAS_NUM_THREADS'] = '1'
import numpy as np
import pysam
from matplotlib import pyplot as plt
jn = os.path.join

TEST = False
#N_TEST_CHUNKS = 2
#START = int(1e6)
#END = int(1e6 + 1e5)

#run with

localrules: all,flagstat, min_max_dp, get_mean_coverage, get_filter_stats, check_and_remove_problem_crams_bams, check_rename_cram_sample_names, \
             write_cram_list, merge_chrom_filter_files, call_all_sites_vcf  #added call_all_sites_vcf as it takes more than 3 days to run


#plot_individual_filter_stats, plot_site_filter_stats,

## Config
configfile: "config_Pti1.yaml"
ana_dir = config['ana_dir']
results_dir = config['results_dir']

################### Global parameters ############################


#dtype=str is important for correct functioning!

#This is important, because otherwise the wildcards in the rules are ambiguous
wildcard_constraints:
    i="\d+",
    contig="\d+",
    chrom="\w+\.1",
    ref_fn=".*.fa",
    #sequence_id="\w+",
    ind_filter_id="[^.]+",
    site_filter_id="[^.]+",
    family_id="\w+",
    filter_id="[^.]+",#"^/(?!raw|all_sites)([^.]+)$"
    chunk="\d+",
    vcf_type="(pass\.snps\.biallelic|pass\.indels\.biallelic|variants)",
    vcf_extension="(vcf|vcf\.gz|bcf)",
    regenotype="(whatshap_regenotype|no_regenotype)",
    alignment_file=".*(sam|cram|bam)"
            #"[^.]+",

#exclude = []
sequence_mt = pd.read_csv(config["sample_mt"], dtype=str, sep='\t', index_col=0).fillna('')#.set_index("sequence_id", drop=False)
#print(sequence_mt)
#sample_mt.drop_duplicates(inplace=True)
#sample_mt = sample_mt[sample_mt['simple_id'].apply(lambda s: s.startswith('MayZeb1'))]
#sample_mt = sample_mt.drop(exclude)
#sample_mt = sample_mt.iloc[:10]
SAMPLES = sequence_mt["individual_id"].unique()
#SAMPLES = ['OP8717', 'SA4095', 'SA8715', 'TL8714', 'WA1315']
#SAMPLES = sample_mt.index.values
n_samples = len(SAMPLES)
chrom_names = pd.read_csv(config["chrom_names"], dtype=str, sep='\t', index_col=0).fillna('')
chrom_names_nucl = pd.read_csv(config["chrom_names_nucl"], dtype=str, sep='\t', index_col=0).fillna('')


data_dir = config['data_dir']
ref_name = config['ref']['name']
ref_base = config['ref']['base_fn']
ref_ext = config['ref']['ext_fa']
ref_indexes = ['.amb', '.ann', '.bwt', '.pac','.sa','.fai']




#filter_id = ["TestFilters20210315"]
#stat_type = ['snp']
#stat_type = list(config["filter_sets"][filter_id].keys())
ref_name = config['ref']['name']
callset_id = config['callset_id']
ind_filter_id = 'no_if1'
site_filter_id = 'sf_stringent1'
#filter_id = 'bla'


data_dir = config['data_dir']
ref_base = config['ref']['base_fn']
ref_ext = config['ref']['ext_fa']
ref_indexes = ['.amb', '.ann', '.bwt', '.pac','.sa','.fai']

#vcf_type = 'variants'
vcf_type = 'pass.snps.biallelic'


#chunk_size = 5e4 #define chunk size here 

chrom_length = pd.read_csv(config['ref']['base_fn'] + config['ref']['ext_fai'],
                           sep='\t', usecols=[0,1], names=['chrom','len'], index_col=0).squeeze()
#chrom_length.squeeze()




#max_chrom_length = chrom_length.max()
#chunk_to_region = {i:(int(i*chunk_size+1),
#                      int((i+1)*chunk_size)) for i in range(int(np.ceil(max_chrom_length/chunk_size)))}
#CHROMOSOMES = chrom_names.index.values
CHROMOSOMES = chrom_names_nucl.index.values

sequence_id = sequence_mt.index.values
#CHROMOSOMES = 'Chr1' #use this to test on one chromosome

##################### rule all ##############################
def get_crams(sequence_id):
    return f"{data_dir}/alignment/{ref_name}/{sequence_id}.crumble.cram",

rule all:
    input:
#        expand(f'{data_dir}/reads/fastqc/{{sample}}_1_fastqc.zip', sample = sequence_id),
#        expand(f'{data_dir}/reads/fastqc/{{sample}}_2_fastqc.zip', sample = sequence_id),
#        f"{ref_base}.fna.amb",
#        f"{ref_base}.fna.ann",
#        f"{ref_base}.fna.bwt",
#        f"{ref_base}.fna.pac",
#        f"{ref_base}.fna.sa",
        #f"{ref_base}.fna.fai",
#        f"{ref_base}.dict",
#        expand(f"{data_dir}/alignment/{ref_name}/{{individual_id}}.fixmate.sort.markdup.rg.bam", individual_id = SAMPLES),
#        expand(f"{data_dir}/alignment/{ref_name}/{{individual_id}}.crumble.cram.flagstat", individual_id = SAMPLES),
#        expand(f"{data_dir}/alignment/{ref_name}/{{individual_id}}.crumble.cram", individual_id = SAMPLES),
        expand(f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{{chrom}}.bcf", chrom = CHROMOSOMES),
 #       expand(f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{{chromm}}.bcf", chromm = CHROMOSOMES),
        expand(f"{ana_dir}/data/{ref_name}/{callset_id}/total_coverage.{{chrom}}.txt", chrom = CHROMOSOMES),
        #f"{ana_dir}/data/{ref_name}/{callset_id}/mean_coverage.txt",
        #ana_dir + '/data/' + f"{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        #ana_dir + '/data/' + f"{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt",
        #ana_dir + '/data/' + f'{ref_name}/{callset_id}/individual_filter_thresholds.{ind_filter_id}.yaml',
 #       expand(f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{{chrom}}.bcf", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/filter_per_sample.{ind_filter_id}.{{chrom}}.tsv", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/total_per_sample.{ind_filter_id}.{{chrom}}.tsv", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/stat_hists.{ind_filter_id}.{{chrom}}.yaml", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/individual_dp_hists.{ind_filter_id}.{{chrom}}.yaml", chrom = CHROMOSOMES),
        f'{ana_dir}/figures/individual_dp_hist.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
        f'{ana_dir}/figures/individual_dp_hist.{ref_name}.{callset_id}.{ind_filter_id}.svg',
        f'{ana_dir}/figures/filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
        f'{ana_dir}/figures/filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.svg',
        ana_dir + '/figures/' + f'stat_hists.{ref_name}.{callset_id}.{ind_filter_id}.{site_filter_id}.pdf',
        ana_dir + '/figures/' + f'stat_hists.{ref_name}.{callset_id}.{ind_filter_id}.{site_filter_id}.svg',
#        expand(f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.bcf", chrom = CHROMOSOMES),
#        expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.info", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf", chrom = CHROMOSOMES),
        ##expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf.csi", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.info.gz", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.bed.gz", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.info.gz.tbi", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.bed.gz.tbi", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.accessible.bed.gz", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{{chrom}}.bcf", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{{chrom}}.bcf.tbi", chrom = CHROMOSOMES),
        #expand(f"{ana_dir}/data/{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{{chrom}}.bcf.csi", chrom = CHROMOSOMES),
        #espand(results_dir  + f"/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf", chrom=CHROMOSOMES),
        #f"{results_dir}/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.all_chrom.filters.info.gz", 
        #f"{results_dir}/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.all_chrom.accessible.bed.gz",
        ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{vcf_type}.all_chrom.vcf.gz",
        ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{vcf_type}.all_chrom.vcf.gz.tbi",
        ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{vcf_type}.all_chrom.vcf.gz.csi",
#

    
def get_read_fn(wildcards):
    file_str = sequence_mt.loc[wildcards.sample,'calcua_read_location']
    file_list = file_str.rsplit(',')
    return file_list

#study_name = sample_mt.loc[sequence_id,'study']

########################### fastqc ################################

##---HS----##
#(1) this does not work: compare the two {}{} commands in the shell rule: 1st) format string 2nd) snakemake replacement
# it is strange that this rule does not take an input filename; this relies on fastqfile being in a specific directory
# would need to be geneneralised
##/--HS----##

rule fastqc:
    input:
        read = get_read_fn,
    output:
        f"{data_dir}/reads/fastqc/{{sample}}_1_fastqc.zip",
        f"{data_dir}/reads/fastqc/{{sample}}_1_fastqc.html",
        f"{data_dir}/reads/fastqc/{{sample}}_2_fastqc.zip",
        f"{data_dir}/reads/fastqc/{{sample}}_2_fastqc.html"
    params:
        output_dir = f"{data_dir}/reads/fastqc/"
    resources:
        walltime=5,
        mem_mb=1000
    run:
       # if os.path.isfile(f"{data_dir}/reads/{wildcards.sample}.fq.gz"):
        #    input = f"{data_dir}/reads/{wildcards.sample}.fq.gz"
        #else:
         #   input = f"{data_dir}/reads/{wildcards.sample}.fastq.gz"
        shell(f"fastqc {input.read} --outdir {params.output_dir}")



############################ bwa_index ################################

rule bwa_index:
    input:
        ref = f"{ref_base}.fna"
    output:
        amb = f"{ref_base}.fna.amb",
        ann = f"{ref_base}.fna.ann",
        bwt = f"{ref_base}.fna.bwt",
        pac = f"{ref_base}.fna.pac",
        sa = f"{ref_base}.fna.sa",
#        fai = f"{ref_base}.fna.fai",
        dict = f"{ref_base}.dict"
    resources:
        walltime = 3,
        mem_mb=5000
    params:
        ref_base = f"{ref_base}"
    shell:
        """
        bwa index {input.ref}
        samtools dict \
            -a {params.ref_base} \
            {input.ref} \
            -o {output.dict}
        """        
#samtools faidx {input.ref}
        

########################### align reads #################################
#
#def get_read_fn(wildcards):
#    file_str = sample_mt.loc[wildcards.sequence_id,'calcua_read_location']
#    file_list = file_str.rsplit(',')
#    return file_list
#
def get_ref_fn(ref_name):
    return config['ref']['base_fn'] + config['ref']['ext_fa']

#for your wildcards.individual_id get a list of all sequence ids of that individual (fish), of the same order as in get_read_fn2
def get_sequence_ids(individual_id):
    #id = wildcards.individual_id
    return sequence_mt.reset_index(drop=False).set_index("individual_id").loc[[individual_id],"sequence_id"].values


#for your wildcards.individual_id get a list of all sequences of that individual (fish), of the form
#def get_read_fn2(wildcards):
#    sequence_ids = get_sequence_ids(wildcards.individual_id)
#    file_list = sequence_mt.loc[sequence_ids,"calcua_read_location"].values
#    file_list = [tuple(s.split(",")) for s in file_list]
#
#    #file_list =  [(forward1.fq.gz, reverse1.fq.gz), (forward2.fq.gz, reverse2.fq.gz), ...
#    return file_list

def get_forward_read(wildcards):
    sequence_ids = get_sequence_ids(wildcards.individual_id)
    file_list = sequence_mt.loc[sequence_ids,"calcua_read_location"].values
    #print(file_list)
    file_list = [s.split(",")[0] for s in file_list]
#file_list =  [(forward1.fq.gz, reverse1.fq.gz), (forward2.fq.gz, reverse2.fq.gz), ...
    return file_list

def get_reverse_read(wildcards):
    sequence_ids = get_sequence_ids(wildcards.individual_id)
    file_list = sequence_mt.loc[sequence_ids,"calcua_read_location"].values
    file_list = [s.split(",")[1] for s in file_list]
#file_list =  [(forward1.fq.gz, reverse1.fq.gz), (forward2.fq.gz, reverse2.fq.gz), ...
    return file_list



rule align_reads2:
   input:
       forward_reads = ancient(get_forward_read),
       reverse_reads = ancient(get_reverse_read),
       ref = f"{ref_base}.fna",
       amb = f"{ref_base}.fna.amb"
   output:
       bam = f"{data_dir}/alignment/{ref_name}/{{individual_id}}.fixmate.sort.markdup.rg.bam"
   threads: 28 #12
   resources:
       mem_mb=100000,#lambda wildcards, threads: threads*2,
       walltime = 72
   params:
       sample_id = SAMPLES,
       add_threads = lambda wildcards, threads: threads-1,
       output_bam_prefix = f"{data_dir}/alignment/{ref_name}/{{individual_id}}",
       #ext = lambda wildcards: sample_mt.loc[wildcards['sequence_id'],'calcua_read_location'].rsplit('.',1)[-1],
   run:
       #s = sample_mt.loc[wildcards['sequence_id']]
       #read_files = input.reads
       # don't use -M in bwa mem!! (and also not -P :) )

        
       individual_id = wildcards.individual_id
       intermediate_read_files = []
       for i, (sequence_id, forward, reverse) in enumerate( zip(get_sequence_ids(individual_id), input.forward_reads, input.reverse_reads) ):
             
           rg = f'"@RG\\tID:{sequence_id}\\tSM:{individual_id}\\tPL:BGIseq"'
           rf_n = '{}_{}.tmp'.format(output.bam, sequence_id)
           intermediate_read_files.append(rf_n)
           c0 = (f'bwa mem -t {{threads}} -R {rg} {{input.ref}} {forward} {reverse} '
                ' | samtools fixmate --threads {params.add_threads} -m - - '
                ' | samtools sort -T {params.output_bam_prefix}.sort.tmp --threads {params.add_threads} > ' + rf_n )
           shell(c0)
       shell("samtools merge -  {} | samtools markdup -T {{params.output_bam_prefix}}.markdup.tmp --threads {{params.add_threads}} - {{output.bam}};".format(' '.join(intermediate_read_files)))
       shell('samtools index {output.bam}')

###################### bam statistics #####################################

#HS: Add a crumble cram step

rule bam_to_cram:
    input:
        bam = f"{data_dir}/alignment/{ref_name}/{{individual_id}}.fixmate.sort.markdup.rg.bam",
        ref = f"{ref_base}.fna"
    output:
        CC = f"{data_dir}/alignment/{ref_name}/{{individual_id}}.crumble.cram",
    threads: 1
    resources:
        mem_mb=14000,# lambda wildcards, threads: threads*2,
        walltime = 24
    shell:
        """
        export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/data/antwerpen/grp/asvardal/miniconda3/envs/hscon5/lib"
        /data/antwerpen/grp/asvardal/software/crumble/crumble -O cram,reference={input.ref},lossy_names {input.bam} {output.CC}
        samtools index {output.CC}
        samtools quickcheck -v {output.CC}
        rm {input.bam}
        """
rule flagstat:
    input:
         cram = f"{data_dir}/alignment/{ref_name}/{{individual_id}}.crumble.cram"
    output:
        flagstat = f"{data_dir}/alignment/{ref_name}/{{individual_id}}.crumble.cram.flagstat"
    threads: 1
    resources:
        mem_mb=1000,
        walltime=1
    shell:
        'samtools flagstat {input.cram} > {output.flagstat}'


######################### Variant Calling ##################################

######################## bcf all sites calling & statistics ###################################


rule call_all_sites_vcf:
    input:
        crams = [get_crams(id) for id in SAMPLES],
        ref = f"{ref_base}.fna"
    output:
        bcf= f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{{chrom}}.bcf",
        tot_dp= f"{ana_dir}/data/{ref_name}/{callset_id}/total_coverage.{{chrom}}.txt",
    threads: 5
    resources:
        mem_mb=20000,#lambda wildcards, threads: threads*2,
        walltime=72
    params:
        regions = lambda wildcards: wildcards.chrom
        #regions = lambda wildcards: wildcards.chrom + (f":{START}-{END}" if TEST else "")
    shell:
        """
        samtools quickcheck -v {input.crams}
        bcftools mpileup -a FORMAT/AD,FORMAT/DP -d 3000 --threads {threads} -Ou \
            --regions {params.regions} \
            -f {input.ref} \
            {input.crams} \
        | bcftools call --threads {threads} -f GQ -m -Ou \
        | bcftools +fill-tags --threads {threads} -Ob \
         | tee {output.bcf} | bcftools query -f '[%DP\t]\n'  \
         | awk '{{for(i=1; i<=NF;i++) dp_tot[i]+=$i}} END{{for(i=1; i<=NF;i++) {{printf dp_tot[i]"\\t"}}; printf "\\n"}}' \
          > {output.tot_dp}; \
        bcftools index {output.bcf}
        """

##def get_chunks(chrom):
##    chunks = range(int(np.ceil(chrom_length[chrom] / chunk_size)))
##    if TEST:
##        mid_chunk = len(chunks) // 2
##        chunks = [c for c in chunks][mid_chunk:mid_chunk + N_TEST_CHUNKS]
##    return  chunks
#
#
def get_total_dp_files(wildcards):
    cov_files = []
    for chrom in CHROMOSOMES:
        fn = ana_dir + '/data/' + f"{ref_name}/{callset_id}/total_coverage.{chrom}.txt"
        cov_files.append(fn)
    return cov_files

def write_dp(fn, samples, dps):
    with open(fn,'w') as f:
        for sample, dp in zip(samples, dps):
            f.write(sample + '\t' + str(dp) + '\n')

def read_dp(fn):
    #load mean depth dictionary
    dp_dic = {}
    with open(fn) as f:
        for line in f.readlines():
            sample, dp = line.strip().split()
            dp_dic.update({sample: float(dp)})
    return dp_dic



rule get_mean_coverage:
    input:
        total_dps = get_total_dp_files
    output:
        cov = f"{ana_dir}/data/{ref_name}/{callset_id}/mean_coverage.txt"
    run:
        mean_dps = [0 for i in range(n_samples)]
        for fn in input.total_dps:
            with open(fn) as f:
                for i, d in enumerate(f.readline().strip().split()):
                    mean_dps[i] += float(d)
        mean_dps = np.array(mean_dps) / (chrom_length.loc[CHROMOSOMES].sum() if not TEST else N_TEST_CHUNKS*chunk_size*len(CHROMOSOMES))
        print(mean_dps,mean_dps[0])
        dp_dic = {s:float(c) for s,c in zip(SAMPLES, mean_dps)}
        yaml.dump(dp_dic, open(output.cov,'w'))



bins = {'MQ': np.linspace(0,61,62),
                'DP': None, # will be set based on mean_dp below
                'MQ0F': np.linspace(0,1,101),
                'MQSB': np.linspace(0,1,101),
                'DD': np.linspace(0,10,401),
                'ExcHetOrig': np.linspace(0,1,101),
                'AB_Het': np.linspace(0,100,401),
                'MF': None } # will be set based on sample_number below




rule min_max_dp:
    input:
        cov = ana_dir + '/data/' + f"{ref_name}/{callset_id}/mean_coverage.txt"
    output:
        min_dp = ana_dir + '/data/' + f"{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        max_dp = ana_dir + '/data/' + f"{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt",
        individual_filter_thresholds = ana_dir + '/data/' + f'{ref_name}/{callset_id}/individual_filter_thresholds.{ind_filter_id}.yaml',
    params:
        filter_thresh =lambda wildcards: config['individual_filter_sets']['no_if1'],
    run:
        #mean_dp_dic = read_dp(input.cov)
        with open(input.cov) as f:
            mean_dp_dic = yaml.safe_load(f)
        #calculate dictionaries for min/max coverage
        # per sample based on Poisson dist and pvals defined under params
        min_dp = {}
        max_dp = {}
        for sample,dp in mean_dp_dic.items():
            #get the depth thresholds corresponding to <> ind_dp_to_missing_pval of low/excessice coverage
            min_dp.update({sample:float(stats.poisson.ppf(params.filter_thresh['min_dp_to_missing_pval'], dp))})
            max_dp.update({sample:float(stats.poisson.isf(params.filter_thresh['max_dp_to_missing_pval'], dp))})
        #print(max_dp, type(max_dp['CopAnsF17350449']))
        #raise
        yaml.dump(min_dp,open(output.min_dp,'w'))
        yaml.dump(max_dp,open(output.max_dp,'w'))

        #write_dp(output.min_dp, mean_dp_dic.keys(), min_dp)
        #write_dp(output.max_dp,mean_dp_dic.keys(), max_dp)

        filter_thresholds = {s: {'allele_balance': params.filter_thresh['allele_balance'],
                                 'low_depth': min_dp[s],
                                 'high_depth': max_dp[s],
                                 'low_gq': params.filter_thresh['low_gq']} for s in SAMPLES}
        yaml.dump(filter_thresholds, open(output.individual_filter_thresholds,'w'))


rule get_filter_stats:
    """
    This rule calculates filter stats and
    sets individual genotypes to missing according
    thresholds defined in the config file.
    """
    input:
        bcf =  f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{{chrom}}.bcf",
        cov =  ana_dir + '/data/' + f"{ref_name}/{callset_id}/mean_coverage.txt",
        min_dp = ana_dir + '/data/' + f"{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        max_dp = ana_dir + '/data/' + f"{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt"
    output:
        bcf =  f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{{chrom}}.bcf",
        filter_per_sample  =  f"{ana_dir}/data/{ref_name}/{callset_id}/filter_per_sample.{ind_filter_id}.{{chrom}}.tsv",
        total_per_sample =  f"{ana_dir}/data/{ref_name}/{callset_id}/total_per_sample.{ind_filter_id}.{{chrom}}.tsv",
        stat_hists =  f"{ana_dir}/data/{ref_name}/{callset_id}/stat_hists.{ind_filter_id}.{{chrom}}.yaml",
        individual_dp_hists =  f"{ana_dir}/data/{ref_name}/{callset_id}/individual_dp_hists.{ind_filter_id}.{{chrom}}.yaml"
        #**{stat:f'{stat}_{{chrom}}.npy' for stat in bins.keys()},
        #**{sample:f'dp_{sample}_{{chrom}}.npy' for sample in samples}
    threads: 1
    resources:
        mem_mb = 4500,#lambda wildcards, threads: threads*2,
        #walltime = 72#might need to be higher for large samples
    params:
        filter_thresh=lambda wildcards: config['individual_filter_sets']['no_if1'],
        flush_interval = 1e5,
        #regions = lambda wildcards: ((wildcards.chrom, START, END) if TEST else [])
        regions = lambda wildcards: (wildcards.chrom)

    run:
        def flush_data():
            for stat, b in bins.items():
                dt = np.array(temp_data[stat])
                dt[dt > b[-1]] = b[-1]
                dt[dt < b[0]] = b[0]
                h, _ = np.histogram(dt,bins=b)
                hists[stat] += h
                temp_data[stat] = []
            for sample, b in dp_per_sample_bins.items():
                dt = np.array(dp_per_sample_tmp[sample])
                dt[dt > b[-1]] = b[-1]
                dt[dt < b[0]] = b[0]
                h, _ = np.histogram(dt,bins=b)
                dp_per_sample_hists[sample] += h
                dp_per_sample_tmp[sample] = []


        #input file stream
        bcf_in = pysam.VariantFile(input.bcf)  # auto-detect input format
        samples1 = [s for s in bcf_in.header.samples]
        for i, (s0, s1) in enumerate(zip(SAMPLES, samples1)):
            assert s0==s1,  ("Sample at position {} in metadata and vcf are not the same."
                                        "Metadata {} != VCF {}. That can lead to unexpected behaviour.".format(i,s0, s1))

        n_samples1 = len(SAMPLES)
        assert n_samples == n_samples1, "Number of samples in metadata ({}) and VCF ({}) not the same:".format(n_samples, n_samples1)
        bins['MF'] = np.linspace(0, 1, n_samples + 1)

        #this is an output stream to recalculate AN, AC etc after setting some GTs to ./. below
        filltag_stream = subprocess.Popen(['bcftools +fill-tags -Ob'],
            stdin=subprocess.PIPE,
            stdout=open(output.bcf, 'w'),
            stderr=subprocess.PIPE,shell=True,
            encoding='utf8')


        #load mean depth dictionary
        with open(input.cov) as f:
            mean_dp_dic = yaml.safe_load(f)
        total_dp = np.sum([v for v in mean_dp_dic.values()])
        bins['DP'] = np.arange(0,4 * total_dp, 1)

        #calculate dictionaries for min/max coverage
        # per sample based on Poisson dist and pvals defined under params
        with open(input.min_dp) as f:
            min_dp_dic = yaml.safe_load(f)
        with open(input.max_dp) as f:
            max_dp_dic = yaml.safe_load(f)

        #new info annotations calculated below added to vcf header
        bcf_in.header.info.add('DD',1,'Float',
            'Average across samples of deviation from individual mean coverage in units of standard deviation.')
        bcf_in.header.info.add('ExcHetOrig','A','Float',
            'Excess Heterozygosity before genotype filtering.')
        bcf_in.header.info.add('AB_Het',1,'Float',
            'Phred-scaled p-value of binomial test for allele balance violation in heterozygous genotypes. Larger is worse.')
        bcf_in.header.info.add('MF',1,'Float',
            'Fraction of missing (./.) genotypes after genotype filtering.')

        #output object, note how this is piped into filltag_stream for recalcuating INFO tags
        bcf_out = pysam.VariantFile(filltag_stream.stdin,'wu', header=bcf_in.header)

        #filter stats
        filter_per_sample = {s: {'allele_balance': 0,
                                 'low_depth': 0,
                                 'high_depth': 0,
                                 'low_gq': 0} for s in SAMPLES}
        total_per_sample = copy.deepcopy(filter_per_sample)




        # the below defines objects to store indoividual depth values
        # and calculate and sum histograms each flush interval
        dp_per_sample_tmp = {s: [] for s in SAMPLES}
        dp_per_sample_bins = {s: np.arange(0,5 * mean_dp_dic[s],1) for s in SAMPLES}
        dp_per_sample_hists = {k: np.zeros(len(v) - 1) for k, v in dp_per_sample_bins.items()}

        # the below defines objects to store INFO column statistics per line
        # and calculate and sum histograms each flush interval

        hists = {k: np.zeros(len(v) - 1) for k, v in bins.items()}
        temp_data = {k: [] for k in bins.keys()}

        print(SAMPLES)
        for i, rec in enumerate(bcf_in.fetch(params.regions)):
            deviations = []
            ads_hets = np.array([0, 0])
            missing = 0
            for n, s in rec.samples.items():
                dp = s['DP']
                if (dp <= min_dp_dic[n]):
                    s['GT'] = (None, None)
                    filter_per_sample[n]['low_depth'] += 1
                total_per_sample[n]['low_depth'] += 1

                if (dp >= max_dp_dic[n]):
                    s['GT'] = (None, None)
                    filter_per_sample[n]['high_depth'] += 1
                total_per_sample[n]['high_depth'] += 1

                #test for het
                if s['GT'][0] != s['GT'][1]:
                    ad = np.array(s['AD'])[list(s['GT'])]
                    ads_hets += ad
                    ab_pval = stats.binom_test(ad)
                    if ab_pval <= params.filter_thresh['allele_balance']:
                        s['GT'] = (None, None)
                        filter_per_sample[n]['allele_balance'] += 1
                    total_per_sample[n]['allele_balance'] += 1

                deviation = np.abs(s['DP'] - mean_dp_dic[n]) / np.sqrt(mean_dp_dic[n])
                deviations.append(deviation)
                try:
                    if s['GQ'] < params.filter_thresh['low_gq']:
                        s['GT'] = (None, None)
                        filter_per_sample[n]['low_gq']+= 1
                    total_per_sample[n]['low_gq'] += 1
                except KeyError:
                    pass

                #add to individual depth list
                dp_per_sample_tmp[n].append(s['DP'])
                if s['GT'] == (None, None):
                    missing += 1
            missing_fraction = missing * 1. / n_samples
            rec.info.update({'MF': missing_fraction})

            #this calculated the mean across individuals of standardised DP deviation
            #large value means that at this site many individuals have a coverage
            #very different from their mean coverage
            mean_deviation = np.mean(deviations)
            rec.info.update({'DD': mean_deviation})

            #if there are any hets, calculate Allele Balance across all samples
            if np.any(ads_hets):
                ab_pval = stats.binom_test(ads_hets)
                ab_phred = -10 * np.log10(ab_pval)
                rec.info.update({'AB_Het': ab_phred})
                rec.info.update({'ExcHetOrig': rec.info['ExcHet']})

            for stat, l in temp_data.items():
                try:
                    st = rec.info[stat]

                    if stat == 'ExcHetOrig':
                        st = st[0]
                    if st is not None:
                        l.append(st)
                except KeyError:
                    pass

            if not (i + 1) % params.flush_interval:
                flush_data()
            #write the line to the output stream
            #print(rec)
            bcf_out.write(rec)
        #add stats to histogram for last interval
        flush_data()

        stat_dic = {}
        for stat, h in hists.items():
            b = bins[stat]
            stat_dic.update({stat:
                                 {'bins': [float(b0) for b0 in b],
                                  'hists': [int(i) for i in h]}})
        yaml.dump(stat_dic, open(output.stat_hists,'w'))

        d = {}
        for sample, h in dp_per_sample_hists.items():
            b = dp_per_sample_bins[sample]
            d.update({sample:
                          {'bins': [float(b0) for b0 in b],
                           'hists': [int(i) for i in h]}})
        yaml.dump(d, open(output.individual_dp_hists,'w'))

        filter_per_sample_df = pd.DataFrame(filter_per_sample)
        filter_per_sample_df.index.name = 'statistic'
        filter_per_sample_df.columns.name = 'sample'
        filter_per_sample_df.to_csv(output.filter_per_sample,sep='\t')

        total_per_sample_df = pd.DataFrame(total_per_sample)
        total_per_sample_df.index.name = 'statistic'
        total_per_sample_df.columns.name = 'sample'
        total_per_sample_df.to_csv(output.total_per_sample, sep='\t')

        bcf_out.close()
        o,e = filltag_stream.communicate()
        if o is not None:
            print(o,file=sys.stdout)
        if e is not None:
            print(e, file=sys.stderr)

rule plot_individual_filter_stats:
    """
    Plots staistics of indiviudal
    genotypes set to ./. because of
    filter thresholds.
    Also plots individual DP distributions
    """
    input:
        #I think the expand needs to be removed here - would this have impacted the output files? 
        filter_per_samples = [ expand(f'{ana_dir}/data/{ref_name}/{callset_id}/filter_per_sample.{ind_filter_id}.{{chrom}}.tsv', chrom = CHROMOSOMES) ],
        total_per_samples = [ expand(f'{ana_dir}/data/{ref_name}/{callset_id}/total_per_sample.{ind_filter_id}.{{chrom}}.tsv', chrom = CHROMOSOMES) ],
        individual_dp_hists = [ expand(f'{ana_dir}/data/{ref_name}/{callset_id}/individual_dp_hists.{ind_filter_id}.{{chrom}}.yaml', chrom = CHROMOSOMES) ],
        min_dp = ana_dir + '/data/' + "{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        max_dp = ana_dir + '/data/' + "{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt",
        individual_filter_thresholds = ana_dir + '/data/' + '{ref_name}/{callset_id}/individual_filter_thresholds.{ind_filter_id}.yaml',
    output:
        ind_dp_hist_pdf =  ana_dir + '/figures/' +'individual_dp_hist.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
        ind_dp_hist_svg =  ana_dir + '/figures/' +'individual_dp_hist.{ref_name}.{callset_id}.{ind_filter_id}.svg',
        filter_per_sample_pdf =  ana_dir + '/figures/' + 'filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
        filter_per_sample_svg = ana_dir + '/figures/' + 'filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.svg',
    #params:
    #    filter_thresh = lambda wildcards: config['individual_filter_sets'][wildcards.ind_filter_id],
    run:
        #Bar plots of how many genotypes per sample were set to mission because
        #of each of the filters
        filtered = pd.read_csv(input.filter_per_samples[0], sep='\t', index_col=0)
        total = pd.read_csv(input.total_per_samples[0],sep='\t',index_col=0)

        for fn1, fn2 in zip(input.filter_per_samples[1:], input.total_per_samples[1:]):
            f = pd.read_csv(fn1, sep='\t', index_col=0)
            t = pd.read_csv(fn2, sep='\t', index_col=0)
            filtered += f
            total += t
        rel_filter = filtered / total

        with open(input.individual_filter_thresholds) as f:
            filter_thresholds = yaml.safe_load(f)

        n_cols = 1
        n_rows = int(np.ceil(len(filtered) / n_cols))
        fig = plt.figure(figsize=(0.2 * n_samples, 4 * n_rows))
        for i, (stat, f) in enumerate(rel_filter.iterrows()):

            ax = fig.add_subplot(n_rows,n_cols,i + 1)

            if stat in ['low_depth', 'high_depth']:
                f.index = [s + " ({})".format(filter_thresholds[s][stat]) for s in f.index]
                title = stat
            else:
                title = stat + " (threshold {})".format(filter_thresholds[SAMPLES[0]][stat])

            f.plot(kind='bar',ax=ax,legend=False,label='Total')
            ax.set_title(title)
            ax.set_ylabel("Proportion filtered")
        plt.tight_layout()

        plt.savefig(output.filter_per_sample_pdf, bbox_inches='tight')
        plt.savefig(output.filter_per_sample_svg, bbox_inches='tight')

        #dictionaries for min/max coverage
        # per sample based on Poisson dist and pvals defined under params

        with open(input.min_dp) as f:
            min_dp_dic = yaml.safe_load(f)
        with open(input.max_dp) as f:
            max_dp_dic = yaml.safe_load(f)
        #min_dp_dic = read_dp(input.min_dp)
        #max_dp_dic = read_dp(input.max_dp)




        with open(input.individual_dp_hists[0]) as f:
            sd = yaml.safe_load(f)
            bins = {s:v['bins'] for s,v in sd.items()}
            hists = {s:np.array(v['hists']).astype(int) for s,v in sd.items()}
        for fn in input.individual_dp_hists[1:]:
            with open(fn) as f:
                sd = yaml.safe_load(f)
                bins1 = {s: v['bins'] for s, v in sd.items()}
                hists1 = {s: np.array(v['hists']).astype(int) for s, v in sd.items()}
                assert bins1 == bins
                for s,h in hists1.items():
                    hists[s] += h
        bins = {s:np.array(v) for s,v in bins.items()}

        fig = plt.figure(figsize=(16,5*int(np.ceil(len(hists) / 3))))
        for i,(sample, h) in enumerate(hists.items()):
            #ax = fig.add_subplot(np.ceil(len(hists) / 3), 3, i+1)
            ax = fig.add_subplot(int(np.ceil(len(hists) / 3)), 3, i+1)
            #poss = bins[sample][:-1] + (bins[sample][1:] - bins[sample][:-1]) / 2
            plt.bar(bins[sample][:-1], h, width=bins[sample][1:] - bins[sample][:-1], align='edge')
            mn = min_dp_dic[sample]
            mx = max_dp_dic[sample]
            plt.axvspan(bins[sample][0], mn, color='r', alpha=0.2)
            plt.axvspan(mx, bins[sample][-1], color='r', alpha=0.2)
            ax.set_xlabel(sample)
        plt.savefig(output.ind_dp_hist_pdf, bbox_inches='tight')
        plt.savefig(output.ind_dp_hist_svg, bbox_inches='tight')



rule plot_site_filter_stats:
    """
    Plots filter statistic distributions with 
    site filter threshold.
    And creates a dynamic site filters to translate
    relative filters into absolute values. 
    """
    input:
        stat_hists = expand(f"{ana_dir}/data/{ref_name}/{callset_id}/stat_hists.{ind_filter_id}.{{chrom}}.yaml", chrom = CHROMOSOMES)
    output:
        #dynamic_filter_config = f'{ana_dir}/_data/{{ref_name}}/{{callset_id}}/stat_hists.{{ind_filter_id}}.{{site_filter_id}}.yaml',
        stat_hist_pdf =  ana_dir + '/figures/' +'stat_hists.{ref_name}.{callset_id}.{ind_filter_id}.{site_filter_id}.pdf',
        stat_hist_svg =  ana_dir + '/figures/' +'stat_hists.{ref_name}.{callset_id}.{ind_filter_id}.{site_filter_id}.svg'
    params:
        filters = lambda wildcards: config['site_filter_sets']['sf_stringent1']['filters'],
    run:
        ##
        with open(input.stat_hists[0]) as f:
            sd = yaml.safe_load(f)
            bins = {s:v['bins'] for s,v in sd.items()}
            hists = {s:np.array(v['hists']).astype(int) for s,v in sd.items()}
        for fn in input.stat_hists[1:]:
            with open(fn) as f:
                sd = yaml.safe_load(f)
                bins1 = {s: v['bins'] for s, v in sd.items()}
                hists1 = {s: np.array(v['hists']).astype(int) for s, v in sd.items()}
                assert bins1 == bins
                for s,h in hists1.items():
                    hists[s] += h

        bins = {s:np.array(v) for s,v in bins.items()}
        n_rows = int(np.ceil(len(params.filters) / 2))
        fig = plt.figure(figsize=(16,5*n_rows))

        for i, (filter_name, fdic) in enumerate(params.filters.items()):
            ax = fig.add_subplot(n_rows, 2, i+1)
            k = fdic['tag']
            try:
                h = hists[k]
            except KeyError as e:
                print("No tag data recorded for filter in config {filter_name} with tag {k}."
                      "You need add this to the bin dict in rule get_filter_stats.")
            plt.bar(bins[k][:-1],h, width=bins[k][1:] - bins[k][:-1], align='edge')

            assert fdic['threshold_type'] == 'absolute', 'relative threshold not yet implemented'
            if fdic['operator'] == '>':
                mn1 = fdic['threshold']
                mx1 = bins[k][-1]
                #pct_filtered = 100 * (ss > threshold).mean()
            elif fdic['operator'] == '<':
                mn1 = bins[k][0]
                mx1 = fdic['threshold']
                #pct_filtered = 100 * (ss < threshold).mean()
            else:
                raise ValueError("Only < and > operators implemented for filters.")
            plt.axvspan(mn1, mx1, color='r', alpha=0.2)
            ax.set_xlabel(k)
            ax.set_title(filter_name)
        plt.savefig(output.stat_hist_pdf, bbox_inches='tight')
        plt.savefig(output.stat_hist_svg, bbox_inches='tight')



######################## apply filters ################################### 


# def get_filter_command(wildcards):
#    commands = []
#    filters = config['site_filter_sets'][wildcards.filter_id]
#
#    for stattype, filter_dic in filter_set.items():
#        filters = filter_dic['filters']
#        dfn = f"{data_dir}/analyses/snakemake/Test_bcftools_variantcalling_20210222/VariantCalling/{callset_id}/dynamic_filter_config_{stattype}_{filter_id}.yaml"
#
#        with open(dfn) as f:
#            dyn_filter_dic = yaml.load(f)
#        filters.update(dyn_filter_dic)
#
#        for i, (filter_name, fd) in enumerate(filters.items()):
#            assert fd['threshold_type'] == 'absolute', ("At this point filter thresholds need to be absolute. Check whether dynamic filter config is correctly produced.")
#            commands.append(f"bcftools filter --soft-filter {filter_name} --mode + -O u --exclude '{fd['tag']} {fd['operator']} {fd['threshold']}' ")
#            #-O u: output-type uncompressed BCF
#            #--mode +: append (instead of overwrite (x)) filters
#
#    return(" | ".join(commands))


def get_filter_command(wildcards):
    commands = []
    filters = config['site_filter_sets']['sf_stringent1']['filters']
    for i, (filter_name, fd) in enumerate(filters.items()):
        assert fd['threshold_type'] == 'absolute', (
            "At this point filter thresholds need to be absolute. Relative thresholds currently not supported.")
        commands.append(
            f"bcftools filter --soft-filter {filter_name} --mode + -O u --exclude '{fd['tag']} {fd['operator']} {fd['threshold']}' ")
    #-O u: output-type uncompressed BCF
    #--mode +: append (instead of overwrite (x)) filters

    return (" | ".join(commands))

rule vcf_add_filters:
    input:
        bcf = f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{{chrom}}.bcf"
    output:
        bcf = ana_dir + '/data/' + f"{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.bcf",
#    group: 'filter_batch'
    params:
        filter_command = get_filter_command
    resources:
        walltime = 6
    shell:
        """
        bcftools view -O u {input.bcf} | \
        {params.filter_command} | \
        bcftools view -O b > {output.bcf}
        """

rule get_filter_info:
    input:
        bcf = f"{ana_dir}/data/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.bcf"
    output:
        filter_info = temp(ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.info"),
#    group: 'filter_batch'
    shell:
        ("""bcftools query --include 'FILTER != "PASS"' """
        "  --format '%CHROM\\t%POS\\t%POS\\t%FILTER\\n' {input.bcf} "
        """ | awk 'BEGIN{{OFS="\t"}} {{print $1,$2,$4}}' > {output.filter_info}  """)


# def get_chunks_per_batch(lst, n, chrom):
#     """Yield successive n-sized chunks from lst."""
#     lst = get_chunks(chrom)
#     for i in range(0, len(lst), n):
#         yield lst[i:i + n]
#
# batch_size = 12
#
# for chrom in CHROMOSOMES:
#     for i, chunks in enumerate(get_chunks_per_batch(samples, batch_size, chrom)):
#       rule:  # batch rule
#         input: [ana_dir + '/_data/' + f"{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{chrom}.{chunk}.bcf",
#                     for chunk in chunks]
#         group: 'filter_batch'
#         threads: batch_size
#         output: touch(f'{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{chrom}.{i}.filter_batch')
#



#rule get_variant_bcf:
#    input:
#        bcfs = lambda wildcards: ana_dir + '/data/' + f"{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.bcf",
#    output:
#        bcf = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf"
#        #bcf_ix = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf.csi",
#    shell:
#        """
#        #bcftools concat -n -Ou {input.bcfs} | \
#        bcftools view -Ob --exclude 'ALT = "."' {input.bcfs}  > {output.bcf};
#       #bcftools index -f {output.bcf}
#        """

## this works but now it does not check if the .bcf.csi index is made anymore
rule get_variant_bcf:
    input:
        bcfs = lambda wildcards: ana_dir + '/data/' + f"{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.bcf",
    output:
        bcf = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf"
    shell:
        """
        bcftools view -Ob --exclude 'ALT = "."' {input.bcfs}  > {output.bcf};
        bcftools index -f {output.bcf}
        """

rule merge_chrom_filter_files:
    input:
        filter_infos = lambda wildcards: ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.info",
    output:
        filter_info = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{chrom}.filters.info.gz",
        filter_bed = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{chrom}.filters.bed.gz",
        filter_info_ix = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{chrom}.filters.info.gz.tbi",
        filter_bed_ix = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{chrom}.filters.bed.gz.tbi",
    params:
        merge_dist=lambda wildcards: config['site_filter_sets']['sf_stringent1']["merge_dist"]
    shell:
        """
        cat {input.filter_infos} | tee >(bgzip -c > {output.filter_info}) \
        | awk  'BEGIN{{OFS="\\t"}} {{print $1,$2-1,$2}}' \
        | bedtools merge -d {params.merge_dist} | bgzip -c  >  {output.filter_bed}; 
         tabix -s 1 -b 2 {output.filter_info}; 
         tabix -p bed {output.filter_bed}; 
        """

rule get_accessible_bed:
    input:
        filter_bed=  ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.bed.gz",
    output:
        accessible_bed = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.accessible.bed.gz",
    params:
        chrom_len = lambda wildcards: chrom_length.loc[wildcards.chrom],
        data_dir = lambda wildcards, output: os.path.dirname(output.accessible_bed)
    shell:
        """
        echo "{wildcards.chrom}\t0\t{params.chrom_len}" > {params.data_dir}/{wildcards.chrom}.bed
        bedtools subtract -a {params.data_dir}/{wildcards.chrom}.bed -b  <(gzip -dc {input.filter_bed}) | bgzip -c > {output.accessible_bed}
        """



#rule copy_callset_results:
#    input:
#        bcfs = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf",
#        filters = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.info.gz",
#        beds = ana_dir + '/data/' + f"{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{{chrom}}.accessible.bed.gz",
#    output:
#        bcfs =  results_dir  + f"/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.variants.{{chrom}}.bcf",
#        filter = results_dir + f"/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.all_chrom.filters.info.gz",
#        filtertab = results_dir + f"/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.all_chrom.filters.info.tabix.gz",
# 	bed = results_dir + f"/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.all_chrom.accessible.bed.gz",
#	bedtab = results_dir + f"/{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.all_chrom.accessible.bed.tabix.gz",
#    params:
#        results_dir = config['results_dir']
#    run:
#        import shutil
#        for bcf, bcfo in zip(input.bcfs,output.bcfs):
#            shutil.copy(bcf,bcfo)
#        shell("cat " + " ".join([f"<(bgzip -dc {b})" for b in input.filters]) + " | bgzip -c > {output.filter}")
#        shell("cat " + " ".join([f"<(bgzip -dc {b})" for b in input.beds])  + " | bgzip -c > {output.bed}")
#        shell("tabix -s 1 -b 2 {output.filtertab}")
#        shell("tabix -p bed {output.bedtab}")



rule select_biallelic_pass_snps:
    input:
        bcf = ana_dir + '/data/' + "{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{chrom}.bcf",
        accessible_bed = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{chrom}.accessible.bed.gz",
    output:
        vcf = ana_dir + '/data/' + "{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.bcf",
        index1 = ana_dir + '/data/' + "{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.bcf.tbi",
        index2 = ana_dir + '/data/' + "{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.bcf.csi",
    resources:
        walltime = 24
    shell:
        """
        bcftools view \
            -Oz \
            --max-alleles 2 \
            --types snps \
            --regions-file {input.accessible_bed}\
            {input.bcf} > {output.vcf};
         bcftools index -f {output.vcf};
         bcftools index --tbi -f {output.vcf}
         """
#print(ana_dir + '/_data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.all_chrom.vcf.gz")

rule merge_chrom_vcfs:
    input:
        #vcfs = expand(ana_dir + '/data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.{vcf_type}.{{chrom}}.bcf", chrom = CHROMOSOMES)
        vcfs = expand(ana_dir + '/data/' + f"{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{{chrom}}.bcf", chrom = CHROMOSOMES)
    output:
        vcf = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{vcf_type}.all_chrom.vcf.gz",
        index1 = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{vcf_type}.all_chrom.vcf.gz.tbi",
        index2 = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.{vcf_type}.all_chrom.vcf.gz.csi",
    resources:
        walltime = 24
    shell:
        """
        bcftools concat -Oz {input.vcfs} > {output.vcf};
        bcftools index -f {output.vcf};
        bcftools index --tbi -f {output.vcf}
        """







## rule concat_vcf_all_chrom:
##     input:
##          vcfs = expand(ana_dir + '/_data/' + "{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{{site_filter_id}}.{{vcf_type}}.{chrom}.{{vcf_extension}}",
##                         chrom=CHROMOSOMES)
##     output:
##         vcf = ana_dir + '/_data/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{vcf_extension}",
##         index1 = ana_dir + '/_data/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{vcf_extension}.tbi",
##         index2 = ana_dir + '/_data/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{vcf_extension}.csi",
##     shell:
##         """
##         bcftools concat \
##             -Oz \
##             {input.vcfs} > {output.vcf};
##          bcftools index -f {output.vcf};
##          bcftools index --tbi -f {output.vcf}
##          """
#
#
#
##rule select_biallelic_pass_indels:
##    input:
##         bcf = ana_dir + '/data/' + "{ref_name}/{callset_id}/{ind_filter_id}.{site_filter_id}.variants.{chrom}.bcf",
##    output:
##        vcf = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.indels.biallelic.{chrom}.vcf.gz",
##        index1 = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.indels.biallelic.{chrom}.vcf.gz.tbi",
##        index2 = ana_dir + '/data/' + "{ref_name}/{callset_id}/{callset_id}.{ind_filter_id}.{site_filter_id}.pass.indels.biallelic.{chrom}.vcf.gz.csi",
##    shell:
##        """
##        bcftools view \
##            -Oz \
##            --max-alleles 2 \
##            --types indels \
##            --apply-filters PASS \
##            {input.bcf} > {output.vcf};
##         bcftools index -f {output.vcf};
##         bcftools index --tbi -f {output.vcf}
##         """
